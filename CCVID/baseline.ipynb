{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ba6702",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7164d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAIN SET Analysis\n",
      "==================================================\n",
      "Total images: 4984\n",
      "Unique person IDs: 75\n",
      "Unique cameras: 6\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 66.5\n",
      "  Median: 66.0\n",
      "  Min: 65\n",
      "  Max: 70\n",
      "  Std: 1.1\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 0 (0.0%)\n",
      "  IDs with >= 20 images: 75 (100.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 836 images, 75 unique persons\n",
      "  Camera 2: 831 images, 75 unique persons\n",
      "  Camera 3: 832 images, 75 unique persons\n",
      "  Camera 4: 827 images, 75 unique persons\n",
      "  Camera 5: 826 images, 75 unique persons\n",
      "  Camera 6: 832 images, 75 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 75 (100.0%)\n",
      "  Average cameras per multi-cam ID: 6.0\n",
      "\n",
      "==================================================\n",
      "QUERY SET Analysis\n",
      "==================================================\n",
      "Total images: 1899\n",
      "Unique person IDs: 151\n",
      "Unique cameras: 6\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 12.6\n",
      "  Median: 9.0\n",
      "  Min: 9\n",
      "  Max: 18\n",
      "  Std: 4.4\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 151 (100.0%)\n",
      "  IDs with >= 20 images: 0 (0.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 453 images, 151 unique persons\n",
      "  Camera 2: 453 images, 151 unique persons\n",
      "  Camera 3: 453 images, 151 unique persons\n",
      "  Camera 4: 180 images, 60 unique persons\n",
      "  Camera 5: 180 images, 60 unique persons\n",
      "  Camera 6: 180 images, 60 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 151 (100.0%)\n",
      "  Average cameras per multi-cam ID: 4.2\n",
      "\n",
      "==================================================\n",
      "GALLERY SET Analysis\n",
      "==================================================\n",
      "Total images: 8397\n",
      "Unique person IDs: 151\n",
      "Unique cameras: 6\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 55.6\n",
      "  Median: 66.0\n",
      "  Min: 31\n",
      "  Max: 72\n",
      "  Std: 16.4\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 0 (0.0%)\n",
      "  IDs with >= 20 images: 151 (100.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 1677 images, 151 unique persons\n",
      "  Camera 2: 1684 images, 151 unique persons\n",
      "  Camera 3: 1679 images, 151 unique persons\n",
      "  Camera 4: 1122 images, 100 unique persons\n",
      "  Camera 5: 1116 images, 100 unique persons\n",
      "  Camera 6: 1119 images, 100 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 151 (100.0%)\n",
      "  Average cameras per multi-cam ID: 5.0\n",
      "\n",
      "==================================================\n",
      "Data Consistency Check\n",
      "==================================================\n",
      "Train IDs: 75\n",
      "Query IDs: 151\n",
      "Gallery IDs: 151\n",
      "\n",
      "Overlaps (should be non-zero for proper evaluation):\n",
      "  Train ∩ Query: 0\n",
      "  Train ∩ Gallery: 0\n",
      "  Query ∩ Gallery: 151 (should equal Query IDs)\n",
      "✓ All query IDs exist in gallery (correct)\n",
      "\n",
      "==================================================\n",
      "SUMMARY REPORT\n",
      "==================================================\n",
      "Total images in dataset: 15280\n",
      "\n",
      "Statistics saved to: /home/ika/yzlm/TwinProject/CCVID/dataset_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset_quality(dataset_dir, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Comprehensive analysis of Market1501 format dataset\"\"\"\n",
    "    \n",
    "    # Market1501 naming pattern: PPPP_CC_SSSSSS.jpg\n",
    "    pattern = re.compile(r'(\\d+)_c(\\d+)_')  # Adjusted for your pattern\n",
    "    market_pattern = re.compile(r'(\\d{4})_(\\d{2})_(\\d{6})\\.jpg')  # Standard Market1501\n",
    "    \n",
    "    id_counts = defaultdict(int)\n",
    "    cam_counts = defaultdict(int)\n",
    "    id_cam_pairs = defaultdict(set)\n",
    "    cam_id_pairs = defaultdict(set)\n",
    "    \n",
    "    images = [f for f in os.listdir(dataset_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    for img in images:\n",
    "        # Try Market1501 pattern first\n",
    "        m = market_pattern.search(img)\n",
    "        if m:\n",
    "            person_id = int(m.group(1))\n",
    "            camera_id = int(m.group(2))\n",
    "        else:\n",
    "            # Try your custom pattern\n",
    "            m = pattern.search(img)\n",
    "            if m:\n",
    "                person_id = int(m.group(1))\n",
    "                camera_id = int(m.group(2))\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        id_counts[person_id] += 1\n",
    "        cam_counts[camera_id] += 1\n",
    "        id_cam_pairs[person_id].add(camera_id)\n",
    "        cam_id_pairs[camera_id].add(person_id)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    id_counts_list = list(id_counts.values())\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{dataset_name} Analysis\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    print(f\"Unique person IDs: {len(id_counts)}\")\n",
    "    print(f\"Unique cameras: {len(cam_counts)}\")\n",
    "    \n",
    "    if id_counts:\n",
    "        print(f\"\\nImages per person ID:\")\n",
    "        print(f\"  Average: {np.mean(id_counts_list):.1f}\")\n",
    "        print(f\"  Median: {np.median(id_counts_list):.1f}\")\n",
    "        print(f\"  Min: {min(id_counts_list)}\")\n",
    "        print(f\"  Max: {max(id_counts_list)}\")\n",
    "        print(f\"  Std: {np.std(id_counts_list):.1f}\")\n",
    "        \n",
    "        # Distribution analysis\n",
    "        low_count_ids = [id for id, count in id_counts.items() if count < 5]\n",
    "        medium_count_ids = [id for id, count in id_counts.items() if 5 <= count < 20]\n",
    "        high_count_ids = [id for id, count in id_counts.items() if count >= 20]\n",
    "        \n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(f\"  IDs with < 5 images: {len(low_count_ids)} ({len(low_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        print(f\"  IDs with 5-19 images: {len(medium_count_ids)} ({len(medium_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        print(f\"  IDs with >= 20 images: {len(high_count_ids)} ({len(high_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        \n",
    "        # Camera distribution\n",
    "        print(f\"\\nCamera distribution:\")\n",
    "        for cam_id in sorted(cam_counts.keys()):\n",
    "            print(f\"  Camera {cam_id}: {cam_counts[cam_id]} images, {len(cam_id_pairs[cam_id])} unique persons\")\n",
    "        \n",
    "        # Cross-camera analysis\n",
    "        multi_cam_ids = [id for id, cams in id_cam_pairs.items() if len(cams) > 1]\n",
    "        print(f\"\\nCross-camera statistics:\")\n",
    "        print(f\"  IDs appearing in multiple cameras: {len(multi_cam_ids)} ({len(multi_cam_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        \n",
    "        if multi_cam_ids:\n",
    "            cam_counts_per_id = [len(id_cam_pairs[id]) for id in multi_cam_ids]\n",
    "            print(f\"  Average cameras per multi-cam ID: {np.mean(cam_counts_per_id):.1f}\")\n",
    "    \n",
    "    return id_counts, cam_counts, id_cam_pairs\n",
    "\n",
    "def plot_distribution(id_counts, title=\"Images per Person Distribution\"):\n",
    "    \"\"\"Plot histogram of images per person\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts = list(id_counts.values())\n",
    "    plt.hist(counts, bins=50, edgecolor='black')\n",
    "    plt.xlabel('Number of Images')\n",
    "    plt.ylabel('Number of Persons')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def check_data_consistency(train_dir, query_dir, gallery_dir):\n",
    "    \"\"\"Check for data consistency across splits\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Data Consistency Check\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get person IDs from each split\n",
    "    train_ids = set()\n",
    "    query_ids = set()\n",
    "    gallery_ids = set()\n",
    "    \n",
    "    pattern = re.compile(r'(\\d+)_')\n",
    "    \n",
    "    for img in os.listdir(train_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            train_ids.add(int(m.group(1)))\n",
    "    \n",
    "    for img in os.listdir(query_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            query_ids.add(int(m.group(1)))\n",
    "    \n",
    "    for img in os.listdir(gallery_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            gallery_ids.add(int(m.group(1)))\n",
    "    \n",
    "    # Check overlaps\n",
    "    train_query_overlap = train_ids & query_ids\n",
    "    train_gallery_overlap = train_ids & gallery_ids\n",
    "    query_gallery_overlap = query_ids & gallery_ids\n",
    "    \n",
    "    print(f\"Train IDs: {len(train_ids)}\")\n",
    "    print(f\"Query IDs: {len(query_ids)}\")\n",
    "    print(f\"Gallery IDs: {len(gallery_ids)}\")\n",
    "    \n",
    "    print(f\"\\nOverlaps (should be non-zero for proper evaluation):\")\n",
    "    print(f\"  Train ∩ Query: {len(train_query_overlap)}\")\n",
    "    print(f\"  Train ∩ Gallery: {len(train_gallery_overlap)}\")\n",
    "    print(f\"  Query ∩ Gallery: {len(query_gallery_overlap)} (should equal Query IDs)\")\n",
    "    \n",
    "    # Check if query IDs are subset of gallery IDs\n",
    "    if query_ids.issubset(gallery_ids):\n",
    "        print(\"✓ All query IDs exist in gallery (correct)\")\n",
    "    else:\n",
    "        missing = query_ids - gallery_ids\n",
    "        print(f\"✗ {len(missing)} query IDs missing from gallery: {list(missing)[:5]}...\")\n",
    "    \n",
    "    return train_ids, query_ids, gallery_ids\n",
    "\n",
    "def generate_reid_statistics(base_dir):\n",
    "    \"\"\"Generate comprehensive statistics for the converted dataset\"\"\"\n",
    "    train_dir = os.path.join(base_dir, \"bounding_box_train\")\n",
    "    query_dir = os.path.join(base_dir, \"query\")\n",
    "    gallery_dir = os.path.join(base_dir, \"bounding_box_test\")\n",
    "    \n",
    "    # Analyze each split\n",
    "    train_stats = analyze_dataset_quality(train_dir, \"TRAIN SET\")\n",
    "    query_stats = analyze_dataset_quality(query_dir, \"QUERY SET\")\n",
    "    gallery_stats = analyze_dataset_quality(gallery_dir, \"GALLERY SET\")\n",
    "    \n",
    "    # Check consistency\n",
    "    check_data_consistency(train_dir, query_dir, gallery_dir)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    total_images = len(os.listdir(train_dir)) + len(os.listdir(query_dir)) + len(os.listdir(gallery_dir))\n",
    "    print(f\"Total images in dataset: {total_images}\")\n",
    "    \n",
    "    # Save statistics to file\n",
    "    stats_file = os.path.join(base_dir, \"dataset_statistics.txt\")\n",
    "    with open(stats_file, 'w') as f:\n",
    "        f.write(\"CCVID to Market1501 Conversion Statistics\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Total images: {total_images}\\n\")\n",
    "        f.write(f\"Train images: {len(os.listdir(train_dir))}\\n\")\n",
    "        f.write(f\"Query images: {len(os.listdir(query_dir))}\\n\")\n",
    "        f.write(f\"Gallery images: {len(os.listdir(gallery_dir))}\\n\")\n",
    "    \n",
    "    print(f\"\\nStatistics saved to: {stats_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/home/ika/yzlm/TwinProject/CCVID\"\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    generate_reid_statistics(base_dir)\n",
    "    \n",
    "    # Optional: Plot distributions\n",
    "    # train_dir = os.path.join(base_dir, \"bounding_box_train\")\n",
    "    # train_stats, _, _ = analyze_dataset_quality(train_dir, \"TRAIN SET\")\n",
    "    # plot_distribution(train_stats, \"Train Set: Images per Person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a5ef6",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d46b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 17:34:48,106 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2025-07-27 17:34:48,150 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:6.0.0-pyt\n",
      "2025-07-27 17:34:48,159 [TAO Toolkit] [WARNING] nvidia_tao_cli.components.docker_handler.docker_handler 295: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ika/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2025-07-27 17:34:48,159 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 308: Printing tty value True\n",
      "2025-07-27 14:34:50,635 [TAO Toolkit] [INFO] matplotlib.font_manager 1639: generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'train_with_val.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:110: UserWarning: \n",
      "'train_with_val.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/loggers/api_logging.py:236: UserWarning: Log file already exists at /home/ika/yzlm/TwinProject/CCVID/results/train/status.json\n",
      "  rank_zero_warn(\n",
      "Seed set to 1234\n",
      "Train results will be saved at: /home/ika/yzlm/TwinProject/CCVID/results/train\n",
      "Setting resume checkpoint to /home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_000_step_00000.pth\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Train    │      75 │     118613 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Query    │     151 │     116799 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │     151 │     112421 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Train    │      75 │     118613 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Query    │     151 │     116799 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │     151 │     112421 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ika/yzlm/TwinProject/CCVID/results/train exists and is not empty.\n",
      "Restoring states from the checkpoint path at /home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_000_step_00000.pth\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | Baseline           | 24.1 M | train\n",
      "1 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------\n",
      "24.1 M    Trainable params\n",
      "256       Non-trainable params\n",
      "24.1 M    Total params\n",
      "96.209    Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_000_step_00000.pth\n",
      "\n",
      "Epoch 1:  11%|█▏        | 212/1851 [00:19<02:33, 10.68it/s, v_num=1, train_loss_step=1.050, train_loss_epoch=1.600, base_lr=0.000382]"
     ]
    }
   ],
   "source": [
    "!tao model re_identification train -e /home/ika/yzlm/TwinProject/CCVID/train_with_val.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ace39",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85abb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao model re_identification export \\\n",
    "    -e /home/ika/yzlm/TwinProject/CCVID/export.yaml \\\n",
    "    dataset.num_classes=158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation komutu\n",
    "!tao model re_identification evaluate \\\n",
    "    -e /home/ika/yzlm/TwinProject/CCVID/train_with_val.yaml \\\n",
    "    evaluate.checkpoint=/home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_049_step_03838.pth \\\n",
    "    evaluate.query_dataset=/home/ika/yzlm/TwinProject/CCVID/CCVID_proper_split/query \\\n",
    "    evaluate.test_dataset=/home/ika/yzlm/TwinProject/CCVID/CCVID_proper_split/bounding_box_test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation komutu\n",
    "!tao model re_identification evaluate \\\n",
    "    -e /home/ika/yzlm/TwinProject/CCVID/train_with_val.yaml \\\n",
    "    evaluate.checkpoint=/home/ika/yzlm/TwinProject/CCVID/results_imagenet/train/model_epoch_049_step_03838.pth \\\n",
    "    evaluate.query_dataset=/home/ika/yzlm/TwinProject/CCVID/CCVID_proper_split/query \\\n",
    "    evaluate.test_dataset=/home/ika/yzlm/TwinProject/CCVID/CCVID_proper_split/bounding_box_test \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
