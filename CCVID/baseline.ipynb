{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ba6702",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7164d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAIN SET Analysis\n",
      "==================================================\n",
      "Total images: 4984\n",
      "Unique person IDs: 75\n",
      "Unique cameras: 6\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 66.5\n",
      "  Median: 66.0\n",
      "  Min: 65\n",
      "  Max: 70\n",
      "  Std: 1.1\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 0 (0.0%)\n",
      "  IDs with >= 20 images: 75 (100.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 836 images, 75 unique persons\n",
      "  Camera 2: 831 images, 75 unique persons\n",
      "  Camera 3: 832 images, 75 unique persons\n",
      "  Camera 4: 827 images, 75 unique persons\n",
      "  Camera 5: 826 images, 75 unique persons\n",
      "  Camera 6: 832 images, 75 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 75 (100.0%)\n",
      "  Average cameras per multi-cam ID: 6.0\n",
      "\n",
      "==================================================\n",
      "QUERY SET Analysis\n",
      "==================================================\n",
      "Total images: 1899\n",
      "Unique person IDs: 151\n",
      "Unique cameras: 6\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 12.6\n",
      "  Median: 9.0\n",
      "  Min: 9\n",
      "  Max: 18\n",
      "  Std: 4.4\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 151 (100.0%)\n",
      "  IDs with >= 20 images: 0 (0.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 453 images, 151 unique persons\n",
      "  Camera 2: 453 images, 151 unique persons\n",
      "  Camera 3: 453 images, 151 unique persons\n",
      "  Camera 4: 180 images, 60 unique persons\n",
      "  Camera 5: 180 images, 60 unique persons\n",
      "  Camera 6: 180 images, 60 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 151 (100.0%)\n",
      "  Average cameras per multi-cam ID: 4.2\n",
      "\n",
      "==================================================\n",
      "GALLERY SET Analysis\n",
      "==================================================\n",
      "Total images: 8397\n",
      "Unique person IDs: 151\n",
      "Unique cameras: 6\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 55.6\n",
      "  Median: 66.0\n",
      "  Min: 31\n",
      "  Max: 72\n",
      "  Std: 16.4\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 0 (0.0%)\n",
      "  IDs with >= 20 images: 151 (100.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 1677 images, 151 unique persons\n",
      "  Camera 2: 1684 images, 151 unique persons\n",
      "  Camera 3: 1679 images, 151 unique persons\n",
      "  Camera 4: 1122 images, 100 unique persons\n",
      "  Camera 5: 1116 images, 100 unique persons\n",
      "  Camera 6: 1119 images, 100 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 151 (100.0%)\n",
      "  Average cameras per multi-cam ID: 5.0\n",
      "\n",
      "==================================================\n",
      "Data Consistency Check\n",
      "==================================================\n",
      "Train IDs: 75\n",
      "Query IDs: 151\n",
      "Gallery IDs: 151\n",
      "\n",
      "Overlaps (should be non-zero for proper evaluation):\n",
      "  Train ∩ Query: 0\n",
      "  Train ∩ Gallery: 0\n",
      "  Query ∩ Gallery: 151 (should equal Query IDs)\n",
      "✓ All query IDs exist in gallery (correct)\n",
      "\n",
      "==================================================\n",
      "SUMMARY REPORT\n",
      "==================================================\n",
      "Total images in dataset: 15280\n",
      "\n",
      "Statistics saved to: /home/ika/yzlm/TwinProject/CCVID/dataset_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset_quality(dataset_dir, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Comprehensive analysis of Market1501 format dataset\"\"\"\n",
    "    \n",
    "    # Market1501 naming pattern: PPPP_CC_SSSSSS.jpg\n",
    "    pattern = re.compile(r'(\\d+)_c(\\d+)_')  # Adjusted for your pattern\n",
    "    market_pattern = re.compile(r'(\\d{4})_(\\d{2})_(\\d{6})\\.jpg')  # Standard Market1501\n",
    "    \n",
    "    id_counts = defaultdict(int)\n",
    "    cam_counts = defaultdict(int)\n",
    "    id_cam_pairs = defaultdict(set)\n",
    "    cam_id_pairs = defaultdict(set)\n",
    "    \n",
    "    images = [f for f in os.listdir(dataset_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    for img in images:\n",
    "        # Try Market1501 pattern first\n",
    "        m = market_pattern.search(img)\n",
    "        if m:\n",
    "            person_id = int(m.group(1))\n",
    "            camera_id = int(m.group(2))\n",
    "        else:\n",
    "            # Try your custom pattern\n",
    "            m = pattern.search(img)\n",
    "            if m:\n",
    "                person_id = int(m.group(1))\n",
    "                camera_id = int(m.group(2))\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        id_counts[person_id] += 1\n",
    "        cam_counts[camera_id] += 1\n",
    "        id_cam_pairs[person_id].add(camera_id)\n",
    "        cam_id_pairs[camera_id].add(person_id)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    id_counts_list = list(id_counts.values())\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{dataset_name} Analysis\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    print(f\"Unique person IDs: {len(id_counts)}\")\n",
    "    print(f\"Unique cameras: {len(cam_counts)}\")\n",
    "    \n",
    "    if id_counts:\n",
    "        print(f\"\\nImages per person ID:\")\n",
    "        print(f\"  Average: {np.mean(id_counts_list):.1f}\")\n",
    "        print(f\"  Median: {np.median(id_counts_list):.1f}\")\n",
    "        print(f\"  Min: {min(id_counts_list)}\")\n",
    "        print(f\"  Max: {max(id_counts_list)}\")\n",
    "        print(f\"  Std: {np.std(id_counts_list):.1f}\")\n",
    "        \n",
    "        # Distribution analysis\n",
    "        low_count_ids = [id for id, count in id_counts.items() if count < 5]\n",
    "        medium_count_ids = [id for id, count in id_counts.items() if 5 <= count < 20]\n",
    "        high_count_ids = [id for id, count in id_counts.items() if count >= 20]\n",
    "        \n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(f\"  IDs with < 5 images: {len(low_count_ids)} ({len(low_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        print(f\"  IDs with 5-19 images: {len(medium_count_ids)} ({len(medium_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        print(f\"  IDs with >= 20 images: {len(high_count_ids)} ({len(high_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        \n",
    "        # Camera distribution\n",
    "        print(f\"\\nCamera distribution:\")\n",
    "        for cam_id in sorted(cam_counts.keys()):\n",
    "            print(f\"  Camera {cam_id}: {cam_counts[cam_id]} images, {len(cam_id_pairs[cam_id])} unique persons\")\n",
    "        \n",
    "        # Cross-camera analysis\n",
    "        multi_cam_ids = [id for id, cams in id_cam_pairs.items() if len(cams) > 1]\n",
    "        print(f\"\\nCross-camera statistics:\")\n",
    "        print(f\"  IDs appearing in multiple cameras: {len(multi_cam_ids)} ({len(multi_cam_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        \n",
    "        if multi_cam_ids:\n",
    "            cam_counts_per_id = [len(id_cam_pairs[id]) for id in multi_cam_ids]\n",
    "            print(f\"  Average cameras per multi-cam ID: {np.mean(cam_counts_per_id):.1f}\")\n",
    "    \n",
    "    return id_counts, cam_counts, id_cam_pairs\n",
    "\n",
    "def plot_distribution(id_counts, title=\"Images per Person Distribution\"):\n",
    "    \"\"\"Plot histogram of images per person\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts = list(id_counts.values())\n",
    "    plt.hist(counts, bins=50, edgecolor='black')\n",
    "    plt.xlabel('Number of Images')\n",
    "    plt.ylabel('Number of Persons')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def check_data_consistency(train_dir, query_dir, gallery_dir):\n",
    "    \"\"\"Check for data consistency across splits\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Data Consistency Check\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get person IDs from each split\n",
    "    train_ids = set()\n",
    "    query_ids = set()\n",
    "    gallery_ids = set()\n",
    "    \n",
    "    pattern = re.compile(r'(\\d+)_')\n",
    "    \n",
    "    for img in os.listdir(train_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            train_ids.add(int(m.group(1)))\n",
    "    \n",
    "    for img in os.listdir(query_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            query_ids.add(int(m.group(1)))\n",
    "    \n",
    "    for img in os.listdir(gallery_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            gallery_ids.add(int(m.group(1)))\n",
    "    \n",
    "    # Check overlaps\n",
    "    train_query_overlap = train_ids & query_ids\n",
    "    train_gallery_overlap = train_ids & gallery_ids\n",
    "    query_gallery_overlap = query_ids & gallery_ids\n",
    "    \n",
    "    print(f\"Train IDs: {len(train_ids)}\")\n",
    "    print(f\"Query IDs: {len(query_ids)}\")\n",
    "    print(f\"Gallery IDs: {len(gallery_ids)}\")\n",
    "    \n",
    "    print(f\"\\nOverlaps (should be non-zero for proper evaluation):\")\n",
    "    print(f\"  Train ∩ Query: {len(train_query_overlap)}\")\n",
    "    print(f\"  Train ∩ Gallery: {len(train_gallery_overlap)}\")\n",
    "    print(f\"  Query ∩ Gallery: {len(query_gallery_overlap)} (should equal Query IDs)\")\n",
    "    \n",
    "    # Check if query IDs are subset of gallery IDs\n",
    "    if query_ids.issubset(gallery_ids):\n",
    "        print(\"✓ All query IDs exist in gallery (correct)\")\n",
    "    else:\n",
    "        missing = query_ids - gallery_ids\n",
    "        print(f\"✗ {len(missing)} query IDs missing from gallery: {list(missing)[:5]}...\")\n",
    "    \n",
    "    return train_ids, query_ids, gallery_ids\n",
    "\n",
    "def generate_reid_statistics(base_dir):\n",
    "    \"\"\"Generate comprehensive statistics for the converted dataset\"\"\"\n",
    "    train_dir = os.path.join(base_dir, \"bounding_box_train\")\n",
    "    query_dir = os.path.join(base_dir, \"query\")\n",
    "    gallery_dir = os.path.join(base_dir, \"bounding_box_test\")\n",
    "    \n",
    "    # Analyze each split\n",
    "    train_stats = analyze_dataset_quality(train_dir, \"TRAIN SET\")\n",
    "    query_stats = analyze_dataset_quality(query_dir, \"QUERY SET\")\n",
    "    gallery_stats = analyze_dataset_quality(gallery_dir, \"GALLERY SET\")\n",
    "    \n",
    "    # Check consistency\n",
    "    check_data_consistency(train_dir, query_dir, gallery_dir)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    total_images = len(os.listdir(train_dir)) + len(os.listdir(query_dir)) + len(os.listdir(gallery_dir))\n",
    "    print(f\"Total images in dataset: {total_images}\")\n",
    "    \n",
    "    # Save statistics to file\n",
    "    stats_file = os.path.join(base_dir, \"dataset_statistics.txt\")\n",
    "    with open(stats_file, 'w') as f:\n",
    "        f.write(\"CCVID to Market1501 Conversion Statistics\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Total images: {total_images}\\n\")\n",
    "        f.write(f\"Train images: {len(os.listdir(train_dir))}\\n\")\n",
    "        f.write(f\"Query images: {len(os.listdir(query_dir))}\\n\")\n",
    "        f.write(f\"Gallery images: {len(os.listdir(gallery_dir))}\\n\")\n",
    "    \n",
    "    print(f\"\\nStatistics saved to: {stats_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/home/ika/yzlm/TwinProject/CCVID\"\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    generate_reid_statistics(base_dir)\n",
    "    \n",
    "    # Optional: Plot distributions\n",
    "    # train_dir = os.path.join(base_dir, \"bounding_box_train\")\n",
    "    # train_stats, _, _ = analyze_dataset_quality(train_dir, \"TRAIN SET\")\n",
    "    # plot_distribution(train_stats, \"Train Set: Images per Person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a5ef6",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d46b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 19:53:25,843 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2025-07-27 19:53:25,888 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:6.0.0-pyt\n",
      "2025-07-27 19:53:25,896 [TAO Toolkit] [WARNING] nvidia_tao_cli.components.docker_handler.docker_handler 295: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ika/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2025-07-27 19:53:25,897 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 308: Printing tty value True\n",
      "2025-07-27 16:53:28,445 [TAO Toolkit] [INFO] matplotlib.font_manager 1639: generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'train_with_val.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:110: UserWarning: \n",
      "'train_with_val.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/loggers/api_logging.py:236: UserWarning: Log file already exists at /home/ika/yzlm/TwinProject/CCVID/results/train/status.json\n",
      "  rank_zero_warn(\n",
      "Seed set to 1234\n",
      "Train results will be saved at: /home/ika/yzlm/TwinProject/CCVID/results/train\n",
      "Setting resume checkpoint to /home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_019_step_33813.pth\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Train    │      75 │     118613 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Query    │     151 │     116799 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │     151 │     112421 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Train    │      75 │     118613 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Query    │     151 │     116799 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │     151 │     112421 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ika/yzlm/TwinProject/CCVID/results/train exists and is not empty.\n",
      "Restoring states from the checkpoint path at /home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_019_step_33813.pth\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | Baseline           | 24.1 M | train\n",
      "1 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------\n",
      "24.1 M    Trainable params\n",
      "256       Non-trainable params\n",
      "24.1 M    Total params\n",
      "96.209    Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_019_step_33813.pth\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "Epoch 30:  11%|█         | 195/1851 [00:18<02:32, 10.83it/s, v_num=1, train_loss_step=0.776, train_loss_epoch=0.799, base_lr=0.000382]]2025-07-27 20:19:19,159 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 371: Stopping container.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/ika/miniconda3/bin/tao\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/nvidia_tao_cli/entrypoint/tao_launcher.py\"\u001b[0m, line \u001b[35m134\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31minstance.launch_command\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mtask_group,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "        \u001b[1;31mtask,\u001b[0m\n",
      "        \u001b[1;31m^^^^^\u001b[0m\n",
      "        \u001b[1;31margs[2:]\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/nvidia_tao_cli/components/instance_handler/local_instance.py\"\u001b[0m, line \u001b[35m390\u001b[0m, in \u001b[35mlaunch_command\u001b[0m\n",
      "    \u001b[31mdocker_handler.run_container\u001b[0m\u001b[1;31m(command, cli_mounts)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/nvidia_tao_cli/components/docker_handler/docker_handler.py\"\u001b[0m, line \u001b[35m372\u001b[0m, in \u001b[35mrun_container\u001b[0m\n",
      "    \u001b[31mself.stop_container\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/nvidia_tao_cli/components/docker_handler/docker_handler.py\"\u001b[0m, line \u001b[35m443\u001b[0m, in \u001b[35mstop_container\u001b[0m\n",
      "    \u001b[31mself._container.stop\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/docker/models/containers.py\"\u001b[0m, line \u001b[35m436\u001b[0m, in \u001b[35mstop\u001b[0m\n",
      "    return \u001b[31mself.client.api.stop\u001b[0m\u001b[1;31m(self.id, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/docker/utils/decorators.py\"\u001b[0m, line \u001b[35m19\u001b[0m, in \u001b[35mwrapped\u001b[0m\n",
      "    return f(self, resource_id, *args, **kwargs)\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/docker/api/container.py\"\u001b[0m, line \u001b[35m1166\u001b[0m, in \u001b[35mstop\u001b[0m\n",
      "    res = self._post(url, params=params, timeout=conn_timeout)\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/docker/utils/decorators.py\"\u001b[0m, line \u001b[35m46\u001b[0m, in \u001b[35minner\u001b[0m\n",
      "    return f(self, *args, **kwargs)\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/docker/api/client.py\"\u001b[0m, line \u001b[35m224\u001b[0m, in \u001b[35m_post\u001b[0m\n",
      "    return \u001b[31mself.post\u001b[0m\u001b[1;31m(url, **self._set_request_timeout(kwargs))\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/requests/sessions.py\"\u001b[0m, line \u001b[35m637\u001b[0m, in \u001b[35mpost\u001b[0m\n",
      "    return \u001b[31mself.request\u001b[0m\u001b[1;31m(\"POST\", url, data=data, json=json, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/requests/sessions.py\"\u001b[0m, line \u001b[35m589\u001b[0m, in \u001b[35mrequest\u001b[0m\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/requests/sessions.py\"\u001b[0m, line \u001b[35m703\u001b[0m, in \u001b[35msend\u001b[0m\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/requests/adapters.py\"\u001b[0m, line \u001b[35m486\u001b[0m, in \u001b[35msend\u001b[0m\n",
      "    resp = conn.urlopen(\n",
      "        method=request.method,\n",
      "    ...<9 lines>...\n",
      "        chunked=chunked,\n",
      "    )\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/urllib3/connectionpool.py\"\u001b[0m, line \u001b[35m716\u001b[0m, in \u001b[35murlopen\u001b[0m\n",
      "    httplib_response = self._make_request(\n",
      "        conn,\n",
      "    ...<5 lines>...\n",
      "        chunked=chunked,\n",
      "    )\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/urllib3/connectionpool.py\"\u001b[0m, line \u001b[35m468\u001b[0m, in \u001b[35m_make_request\u001b[0m\n",
      "    \u001b[31msix.raise_from\u001b[0m\u001b[1;31m(e, None)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m3\u001b[0m, in \u001b[35mraise_from\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/site-packages/urllib3/connectionpool.py\"\u001b[0m, line \u001b[35m463\u001b[0m, in \u001b[35m_make_request\u001b[0m\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/http/client.py\"\u001b[0m, line \u001b[35m1430\u001b[0m, in \u001b[35mgetresponse\u001b[0m\n",
      "    \u001b[31mresponse.begin\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/http/client.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mbegin\u001b[0m\n",
      "    version, status, reason = \u001b[31mself._read_status\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "                              \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/http/client.py\"\u001b[0m, line \u001b[35m292\u001b[0m, in \u001b[35m_read_status\u001b[0m\n",
      "    line = str(\u001b[31mself.fp.readline\u001b[0m\u001b[1;31m(_MAXLINE + 1)\u001b[0m, \"iso-8859-1\")\n",
      "               \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/ika/miniconda3/lib/python3.13/socket.py\"\u001b[0m, line \u001b[35m719\u001b[0m, in \u001b[35mreadinto\u001b[0m\n",
      "    return \u001b[31mself._sock.recv_into\u001b[0m\u001b[1;31m(b)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!tao model re_identification train -e /home/ika/yzlm/TwinProject/CCVID/train_with_val.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ace39",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85abb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 17:57:28,638 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2025-07-28 17:57:28,682 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:6.0.0-pyt\n",
      "2025-07-28 17:57:28,691 [TAO Toolkit] [WARNING] nvidia_tao_cli.components.docker_handler.docker_handler 295: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ika/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2025-07-28 17:57:28,691 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 308: Printing tty value True\n",
      "2025-07-28 14:57:31,201 [TAO Toolkit] [INFO] matplotlib.font_manager 1639: generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:110: UserWarning: \n",
      "'export.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "%/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/Unsqueeze\"](%/Gather_output_0, %onnx::Unsqueeze_504), scope: nvidia_tao_pytorch.cv.re_identification.model.backbones.baseline.Baseline::ython3.12/dist-packages/nvidia_tao_pytorch/cv/re_identification/model/backbones/baseline.py:210:0eAvgPool2d::gap # /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:1382:0ne::/nvidia_tao_pytorch.cv.re_identification.model.backbones.resnet.ResNet::base/torch.nn.modules.conv.Conv2d::feature # /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:549:0torch/nn/functional.py:1702:0cation/model/backbones/resnet.py:129:0v2d::conv3 # /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:549:0Conv2d::downsample.0 # /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:549:00  %/ConsExport results will be saved at: /results/export\n",
      "%fc_pred : Float(*, 256, strides=[256, 1], requires_grad=1, device=cuda:0) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002, training_mode=0, onnx_name=\"/bottleneck/BatchNormalization\"](%/Reshape_output_0, %bottleneck.weight, %bottleneck.bias, %bottleneck.running_mean, %bottleneck.running_var), scope: nvidia_tao_pytorch.cv.re_identification.model.backbones.baseline.Baseline::/torch.nn.modules.batchnorm.BatchNorm1d::bottleneck # /usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:2822:0  return (%fc_pred)\n",
      "\n",
      "2025-07-28 14:57:38,772 [TAO Toolkit] [WARNING] root 339: Telemetry data couldn't be sent, but the command ran successfully.\n",
      "2025-07-28 14:57:38,772 [TAO Toolkit] [WARNING] root 342: [Error]: 'str' object has no attribute 'decode'\n",
      "2025-07-28 14:57:38,772 [TAO Toolkit] [INFO] root 349: Execution status: PASS\n",
      "2025-07-28 17:57:39,559 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 371: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao model re_identification export \\\n",
    "    -e /home/ika/yzlm/TwinProject/CCVID/export.yaml \\\n",
    "    dataset.num_classes=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b217bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: 7495 symlinks created.\n",
      "Part 2: 7495 symlinks created.\n",
      "Part 3: 7495 symlinks created.\n",
      "Part 4: 7495 symlinks created.\n",
      "Part 5: 7495 symlinks created.\n",
      "Part 6: 7495 symlinks created.\n",
      "Part 7: 7495 symlinks created.\n",
      "Part 8: 7495 symlinks created.\n",
      "Part 9: 7495 symlinks created.\n",
      "Part 10: 7495 symlinks created.\n",
      "Part 11: 7495 symlinks created.\n",
      "Part 12: 7495 symlinks created.\n",
      "Part 13: 7495 symlinks created.\n",
      "Part 14: 7495 symlinks created.\n",
      "Part 15: 7491 symlinks created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def split_gallery_symlink(src_folder, dst_root, n_parts=2):\n",
    "    files = [f for f in os.listdir(src_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    random.shuffle(files)  # Rastgele karıştır\n",
    "    total = len(files)\n",
    "    part_size = (total + n_parts - 1) // n_parts\n",
    "\n",
    "    for i in range(n_parts):\n",
    "        part_dir = os.path.join(dst_root, f\"part_{i+1}\")\n",
    "        os.makedirs(part_dir, exist_ok=True)\n",
    "        part_files = files[i*part_size : (i+1)*part_size]\n",
    "        for f in part_files:\n",
    "            src = os.path.join(src_folder, f)\n",
    "            dst = os.path.join(part_dir, f)\n",
    "            if not os.path.exists(dst):\n",
    "                os.symlink(src, dst)\n",
    "        print(f\"Part {i+1}: {len(part_files)} symlinks created.\")\n",
    "\n",
    "# Kullanım:\n",
    "split_gallery_symlink(\n",
    "    src_folder=\"/home/ika/yzlm/TwinProject/CCVID/data/bounding_box_test\",\n",
    "    dst_root=\"/home/ika/yzlm/TwinProject/CCVID/data/bounding_box_test_split\",\n",
    "    n_parts=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0314f3ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/home/ika/yzlm/TwinProject/CCVID/data/query/0001_c10s1_01_00.jpg' -> '/home/ika/yzlm/TwinProject/CCVID/data/query_splits/part_1/0001_c10s1_01_00.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m src_folder = \u001b[33m\"\u001b[39m\u001b[33m/home/ika/yzlm/TwinProject/CCVID/data/query\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m dst_root = \u001b[33m\"\u001b[39m\u001b[33m/home/ika/yzlm/TwinProject/CCVID/data/query_splits/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43msplit_folder_symlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_parts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36msplit_folder_symlink\u001b[39m\u001b[34m(src_folder, dst_root, n_parts)\u001b[39m\n\u001b[32m     12\u001b[39m part_files = files[i*part_size : (i+\u001b[32m1\u001b[39m)*part_size]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m part_files:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43msymlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPart \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(part_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m symlinks created.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileExistsError\u001b[39m: [Errno 17] File exists: '/home/ika/yzlm/TwinProject/CCVID/data/query/0001_c10s1_01_00.jpg' -> '/home/ika/yzlm/TwinProject/CCVID/data/query_splits/part_1/0001_c10s1_01_00.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def split_folder_symlink(src_folder, dst_root, n_parts=5):\n",
    "    files = [f for f in os.listdir(src_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    files.sort()\n",
    "    total = len(files)\n",
    "    part_size = (total + n_parts - 1) // n_parts\n",
    "\n",
    "    for i in range(n_parts):\n",
    "        part_dir = os.path.join(dst_root, f\"part_{i+1}\")\n",
    "        os.makedirs(part_dir, exist_ok=True)\n",
    "        part_files = files[i*part_size : (i+1)*part_size]\n",
    "        for f in part_files:\n",
    "            os.symlink(os.path.join(src_folder, f), os.path.join(part_dir, f))\n",
    "        print(f\"Part {i+1}: {len(part_files)} symlinks created.\")\n",
    "\n",
    "# Kullanım:\n",
    "src_folder = \"/home/ika/yzlm/TwinProject/CCVID/data/query\"\n",
    "dst_root = \"/home/ika/yzlm/TwinProject/CCVID/data/query_splits/\"\n",
    "split_folder_symlink(src_folder, dst_root, n_parts=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6e1172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 20:51:46,716 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2025-07-27 20:51:46,762 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:6.0.0-pyt\n",
      "2025-07-27 20:51:46,772 [TAO Toolkit] [WARNING] nvidia_tao_cli.components.docker_handler.docker_handler 295: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ika/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2025-07-27 20:51:46,772 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 308: Printing tty value True\n",
      "2025-07-27 17:51:49,486 [TAO Toolkit] [INFO] matplotlib.font_manager 1639: generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'train_with_val.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:110: UserWarning: \n",
      "'train_with_val.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/loggers/api_logging.py:236: UserWarning: Log file already exists at /home/ika/yzlm/TwinProject/CCVID/results/evaluate/status.json\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]Evaluate results will be saved at: /home/ika/yzlm/TwinProject/CCVID/results/evaluate\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Query    │       5 │       3894 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │     151 │       7495 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "Testing: |          | 0/? [00:00<?, ?it/s]╒══════════╤═════════╤════════════╤═════════════╕│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Query    │       5 │       3894 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │     151 │       7495 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 89/89 [00:09<00:00,  9.64it/s]The test features are normalized.The distance matrix is computed using euclidean distance. It is then processed by re-ranking.\n",
      "╒════════════════════╤═════════╕\n",
      "│ Name               │ Score   │\n",
      "╞════════════════════╪═════════╡\n",
      "│ mAP                │ 19.4%   │\n",
      "├────────────────────┼─────────┤\n",
      "│ CMC curve, Rank-1  │ 45.3%   │\n",
      "├────────────────────┼─────────┤\n",
      "│ CMC curve, Rank-5  │ 50.4%   │\n",
      "├────────────────────┼─────────┤\n",
      "│ CMC curve, Rank-10 │ 54.2%   │\n",
      "╘════════════════════╧═════════╛\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 89/89 [01:25<00:00,  1.04it/s]2025-07-27 17:53:22,828 [TAO Toolkit] [WARNING] root 339: Telemetry data couldn't be sent, but the command ran successfully.\n",
      "2025-07-27 17:53:22,828 [TAO Toolkit] [WARNING] root 342: [Error]: 'str' object has no attribute 'decode'\n",
      "2025-07-27 17:53:22,828 [TAO Toolkit] [INFO] root 349: Execution status: PASS\n",
      "2025-07-27 20:53:23,588 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 371: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao model re_identification evaluate \\\n",
    "    -e /home/ika/yzlm/TwinProject/CCVID/train_with_val.yaml \\\n",
    "    evaluate.checkpoint=/home/ika/yzlm/TwinProject/CCVID/results/train/model_epoch_029_step_50729.pth \\\n",
    "    evaluate.query_dataset=/home/ika/yzlm/TwinProject/CCVID/data/query_splits/part_1 \\\n",
    "    evaluate.test_dataset=/home/ika/yzlm/TwinProject/CCVID/data/bounding_box_test_split/part_1   \\\n",
    "    re_ranking.re_ranking=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c97d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5070\n",
      "VRAM: 11.5 GB\n",
      "\n",
      "Loading datasets...\n",
      "Query: 116799 images\n",
      "Gallery: 112421 images\n",
      "\n",
      "Extracting/Loading query features...\n",
      "Loading existing features from: /home/ika/yzlm/TwinProject/CCVID/features/query_features.npz\n",
      "Features loaded from: /home/ika/yzlm/TwinProject/CCVID/features/query_features.npz\n",
      "\n",
      "Extracting/Loading gallery features...\n",
      "Loading existing features from: /home/ika/yzlm/TwinProject/CCVID/features/gallery_features.npz\n",
      "Features loaded from: /home/ika/yzlm/TwinProject/CCVID/features/gallery_features.npz\n",
      "\n",
      "Feature shapes:\n",
      "Query: (116799, 256)\n",
      "Gallery: (112421, 256)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import onnxruntime as ort\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "class ReIDImageDataset:\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        self.img_files.sort()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.444, 0.438, 0.457], [0.288, 0.280, 0.275])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        pid = int(self.img_files[idx].split('_')[0])\n",
    "        return img, pid, self.img_files[idx]\n",
    "\n",
    "def save_features(features, pids, fnames, save_path):\n",
    "    \"\"\"Save extracted features to disk\"\"\"\n",
    "    data = {\n",
    "        'features': features,\n",
    "        'pids': pids,\n",
    "        'fnames': fnames\n",
    "    }\n",
    "    # Use numpy's compressed format for efficiency\n",
    "    np.savez_compressed(save_path, **data)\n",
    "    print(f\"Features saved to: {save_path}\")\n",
    "\n",
    "def load_features(load_path):\n",
    "    \"\"\"Load features from disk\"\"\"\n",
    "    data = np.load(load_path)\n",
    "    features = data['features']\n",
    "    pids = data['pids']\n",
    "    fnames = data['fnames']\n",
    "    print(f\"Features loaded from: {load_path}\")\n",
    "    return features, pids, fnames\n",
    "\n",
    "def extract_features_onnx(onnx_path, dataset, batch_size=32, save_path=None):\n",
    "    \"\"\"Extract features with option to save\"\"\"\n",
    "    \n",
    "    # Check if features already exist\n",
    "    if save_path and os.path.exists(save_path):\n",
    "        print(f\"Loading existing features from: {save_path}\")\n",
    "        return load_features(save_path)\n",
    "    \n",
    "    # Extract features\n",
    "    session = ort.InferenceSession(\n",
    "        onnx_path,\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )   \n",
    "    input_name = session.get_inputs()[0].name\n",
    "    features, pids, fnames = [], [], []\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch_imgs = []\n",
    "        batch_pids = []\n",
    "        batch_fnames = []\n",
    "        for j in range(i, min(i+batch_size, len(dataset))):\n",
    "            img, pid, fname = dataset[j]\n",
    "            batch_imgs.append(img.numpy())\n",
    "            batch_pids.append(pid)\n",
    "            batch_fnames.append(fname)\n",
    "        batch_imgs = np.stack(batch_imgs)\n",
    "        \n",
    "        # ONNX inference\n",
    "        feats = session.run(None, {input_name: batch_imgs.astype(np.float32)})[0]\n",
    "        \n",
    "        # L2 normalize\n",
    "        feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n",
    "        \n",
    "        features.append(feats)\n",
    "        pids.extend(batch_pids)\n",
    "        fnames.extend(batch_fnames)\n",
    "    \n",
    "    features = np.vstack(features)\n",
    "    pids = np.array(pids)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        save_features(features, pids, fnames, save_path)\n",
    "    \n",
    "    return features, pids, fnames\n",
    "\n",
    "def evaluate_gpu_memory_efficient(query_features, gallery_features, query_pids, gallery_pids,\n",
    "                                  batch_size=512, topk=(1, 5, 10)):\n",
    "    \"\"\"Memory-efficient and GPU-accelerated ReID evaluation.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Evaluation device: {device}\")\n",
    "\n",
    "    # Tensörleri GPU'ya taşı\n",
    "    qf = torch.from_numpy(query_features).to(device)\n",
    "    gf = torch.from_numpy(gallery_features).to(device)\n",
    "    q_pids = torch.from_numpy(query_pids).to(device)\n",
    "    g_pids = torch.from_numpy(gallery_pids).to(device)\n",
    "\n",
    "    num_query = qf.shape[0]\n",
    "    max_rank = max(topk)\n",
    "\n",
    "    # Toplam metrikleri tutmak için\n",
    "    total_cmc = torch.zeros(max_rank, device=device)\n",
    "    total_ap = 0.0\n",
    "    valid_queries = 0\n",
    "\n",
    "    for i in tqdm(range(0, num_query, batch_size), desc=\"Evaluating Batches\"):\n",
    "        batch_qf = qf[i : i + batch_size]\n",
    "        batch_q_pids = q_pids[i : i + batch_size]\n",
    "\n",
    "        # GPU üzerinde mesafeleri hesapla\n",
    "        distmat = torch.cdist(batch_qf, gf, p=2)\n",
    "\n",
    "        # Sıralanmış indisleri al\n",
    "        indices = torch.argsort(distmat, dim=1)\n",
    "\n",
    "        # GPU üzerinde eşleşmeleri bul\n",
    "        matches = (g_pids[indices] == batch_q_pids.view(-1, 1)).int()\n",
    "\n",
    "        # Geçerli sorguları bul (galeride en az bir eşleşmesi olanlar)\n",
    "        num_rel = matches.sum(dim=1)\n",
    "        has_match = num_rel > 0\n",
    "        \n",
    "        if not has_match.any():\n",
    "            continue\n",
    "            \n",
    "        valid_queries += has_match.sum().item()\n",
    "\n",
    "        # AP (Average Precision) hesapla\n",
    "        positions = torch.arange(1, matches.shape[1] + 1, device=device).expand_as(matches)\n",
    "        precision = matches.cumsum(dim=1) / positions\n",
    "        ap = (precision * matches).sum(dim=1) / num_rel.clamp(min=1)\n",
    "        total_ap += ap[has_match].sum().item()\n",
    "\n",
    "        # CMC (Cumulative Matching Characteristics) hesapla\n",
    "        cmc = matches.cumsum(dim=1)\n",
    "        cmc[cmc > 1] = 1\n",
    "        total_cmc += cmc[has_match, :max_rank].sum(dim=0)\n",
    "\n",
    "    # Final metrikleri hesapla\n",
    "    mAP = total_ap / valid_queries if valid_queries > 0 else 0.0\n",
    "    cmc_scores = (total_cmc / valid_queries).cpu().numpy()\n",
    "\n",
    "    final_scores = [cmc_scores[k-1] for k in topk]\n",
    "    return mAP, final_scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Paths\n",
    "    onnx_path = \"/home/ika/yzlm/TwinProject/CCVID/results/exported/model24.onnx\"\n",
    "    query_dir = \"/home/ika/yzlm/TwinProject/CCVID/data/query\"\n",
    "    gallery_dir = \"/home/ika/yzlm/TwinProject/CCVID/data/bounding_box_test\"\n",
    "    \n",
    "    # Feature save paths\n",
    "    query_features_path = \"/home/ika/yzlm/TwinProject/CCVID/features/query_features.npz\"\n",
    "    gallery_features_path = \"/home/ika/yzlm/TwinProject/CCVID/features/gallery_features.npz\"\n",
    "    \n",
    "    # Create features directory if not exists\n",
    "    os.makedirs(os.path.dirname(query_features_path), exist_ok=True)\n",
    "    \n",
    "    # Parameters\n",
    "    feature_batch_size = 256  # For ONNX inference\n",
    "    eval_batch_size = 1024    # For GPU distance computation\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    query_dataset = ReIDImageDataset(query_dir)\n",
    "    gallery_dataset = ReIDImageDataset(gallery_dir)\n",
    "    print(f\"Query: {len(query_dataset)} images\")\n",
    "    print(f\"Gallery: {len(gallery_dataset)} images\")\n",
    "    \n",
    "    # Extract or load features\n",
    "    print(\"\\nExtracting/Loading query features...\")\n",
    "    query_features, query_pids, query_fnames = extract_features_onnx(\n",
    "        onnx_path, query_dataset, \n",
    "        batch_size=feature_batch_size,\n",
    "        save_path=query_features_path\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExtracting/Loading gallery features...\")\n",
    "    gallery_features, gallery_pids, gallery_fnames = extract_features_onnx(\n",
    "        onnx_path, gallery_dataset, \n",
    "        batch_size=feature_batch_size,\n",
    "        save_path=gallery_features_path\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFeature shapes:\")\n",
    "    print(f\"Query: {query_features.shape}\")\n",
    "    print(f\"Gallery: {gallery_features.shape}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc695b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Batches: 100%|██████████| 913/913 [00:10<00:00, 84.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.3319\n",
      "Rank-1: 0.6199\n",
      "Rank-5: 0.6811\n",
      "Rank-10: 0.7065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results_path = \"/home/ika/yzlm/TwinProject/CCVID/features/evaluation_results.pkl\"\\nwith open(results_path, \\'wb\\') as f:\\n    pickle.dump(results, f)\\nprint(f\"\\nResults saved to: {results_path}\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bellek verimli GPU ile değerlendirme\n",
    "mAP, cmc_scores = evaluate_gpu_memory_efficient(\n",
    "    query_features, gallery_features,\n",
    "    query_pids, gallery_pids,\n",
    "    batch_size=128,\n",
    "    topk=(1, 5, 10)\n",
    ")\n",
    "\n",
    "print(f\"mAP: {mAP:.4f}\")\n",
    "print(f\"Rank-1: {cmc_scores[0]:.4f}\")\n",
    "print(f\"Rank-5: {cmc_scores[1]:.4f}\")\n",
    "print(f\"Rank-10: {cmc_scores[2]:.4f}\")\n",
    "\n",
    "# Optional: Save results\n",
    "results = {\n",
    "    'mAP': mAP,\n",
    "    'rank1': cmc_scores[0],\n",
    "    'rank5': cmc_scores[1],\n",
    "    'rank10': cmc_scores[2]\n",
    "}\n",
    "\n",
    "\"\"\"results_path = \"/home/ika/yzlm/TwinProject/CCVID/features/evaluation_results.pkl\"\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(f\"\\nResults saved to: {results_path}\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
