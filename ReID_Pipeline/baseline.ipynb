{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ba6702",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7164d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAIN SET Analysis\n",
      "==================================================\n",
      "Total images: 9576\n",
      "Unique person IDs: 14\n",
      "Unique cameras: 12\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 684.0\n",
      "  Median: 204.0\n",
      "  Min: 20\n",
      "  Max: 3615\n",
      "  Std: 993.1\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 0 (0.0%)\n",
      "  IDs with >= 20 images: 14 (100.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 521 images, 7 unique persons\n",
      "  Camera 2: 941 images, 5 unique persons\n",
      "  Camera 3: 436 images, 4 unique persons\n",
      "  Camera 4: 1078 images, 10 unique persons\n",
      "  Camera 5: 798 images, 8 unique persons\n",
      "  Camera 6: 1276 images, 13 unique persons\n",
      "  Camera 7: 229 images, 5 unique persons\n",
      "  Camera 8: 500 images, 5 unique persons\n",
      "  Camera 9: 1674 images, 13 unique persons\n",
      "  Camera 10: 619 images, 5 unique persons\n",
      "  Camera 11: 923 images, 9 unique persons\n",
      "  Camera 12: 581 images, 6 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 13 (92.9%)\n",
      "  Average cameras per multi-cam ID: 6.8\n",
      "\n",
      "==================================================\n",
      "QUERY SET Analysis\n",
      "==================================================\n",
      "Total images: 493\n",
      "Unique person IDs: 13\n",
      "Unique cameras: 12\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 37.9\n",
      "  Median: 8.0\n",
      "  Min: 3\n",
      "  Max: 208\n",
      "  Std: 57.8\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 4 (30.8%)\n",
      "  IDs with 5-19 images: 4 (30.8%)\n",
      "  IDs with >= 20 images: 5 (38.5%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 33 images, 6 unique persons\n",
      "  Camera 2: 30 images, 4 unique persons\n",
      "  Camera 3: 22 images, 3 unique persons\n",
      "  Camera 4: 56 images, 5 unique persons\n",
      "  Camera 5: 27 images, 6 unique persons\n",
      "  Camera 6: 71 images, 13 unique persons\n",
      "  Camera 7: 20 images, 4 unique persons\n",
      "  Camera 8: 24 images, 6 unique persons\n",
      "  Camera 9: 93 images, 13 unique persons\n",
      "  Camera 10: 26 images, 5 unique persons\n",
      "  Camera 11: 59 images, 9 unique persons\n",
      "  Camera 12: 32 images, 6 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 13 (100.0%)\n",
      "  Average cameras per multi-cam ID: 6.2\n",
      "\n",
      "==================================================\n",
      "GALLERY SET Analysis\n",
      "==================================================\n",
      "Total images: 7050\n",
      "Unique person IDs: 13\n",
      "Unique cameras: 12\n",
      "\n",
      "Images per person ID:\n",
      "  Average: 542.3\n",
      "  Median: 102.0\n",
      "  Min: 39\n",
      "  Max: 2718\n",
      "  Std: 795.8\n",
      "\n",
      "Distribution:\n",
      "  IDs with < 5 images: 0 (0.0%)\n",
      "  IDs with 5-19 images: 0 (0.0%)\n",
      "  IDs with >= 20 images: 13 (100.0%)\n",
      "\n",
      "Camera distribution:\n",
      "  Camera 1: 441 images, 6 unique persons\n",
      "  Camera 2: 450 images, 4 unique persons\n",
      "  Camera 3: 214 images, 3 unique persons\n",
      "  Camera 4: 593 images, 5 unique persons\n",
      "  Camera 5: 519 images, 6 unique persons\n",
      "  Camera 6: 1127 images, 13 unique persons\n",
      "  Camera 7: 192 images, 4 unique persons\n",
      "  Camera 8: 361 images, 6 unique persons\n",
      "  Camera 9: 1577 images, 13 unique persons\n",
      "  Camera 10: 377 images, 5 unique persons\n",
      "  Camera 11: 768 images, 9 unique persons\n",
      "  Camera 12: 431 images, 6 unique persons\n",
      "\n",
      "Cross-camera statistics:\n",
      "  IDs appearing in multiple cameras: 13 (100.0%)\n",
      "  Average cameras per multi-cam ID: 6.2\n",
      "\n",
      "==================================================\n",
      "Data Consistency Check\n",
      "==================================================\n",
      "Train IDs: 77\n",
      "Query IDs: 75\n",
      "Gallery IDs: 75\n",
      "\n",
      "Overlaps (should be non-zero for proper evaluation):\n",
      "  Train ∩ Query: 0\n",
      "  Train ∩ Gallery: 0\n",
      "  Query ∩ Gallery: 75 (should equal Query IDs)\n",
      "✓ All query IDs exist in gallery (correct)\n",
      "\n",
      "==================================================\n",
      "SUMMARY REPORT\n",
      "==================================================\n",
      "Total images in dataset: 17119\n",
      "\n",
      "Statistics saved to: /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/data/dataset_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset_quality(dataset_dir, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Comprehensive analysis of Market1501 format dataset\"\"\"\n",
    "    \n",
    "    # Market1501 naming pattern: PPPP_CC_SSSSSS.jpg\n",
    "    pattern = re.compile(r'(\\d+)_c(\\d+)_')  # Adjusted for your pattern\n",
    "    market_pattern = re.compile(r'(\\d{4})_(\\d{2})_(\\d{6})\\.jpg')  # Standard Market1501\n",
    "    \n",
    "    id_counts = defaultdict(int)\n",
    "    cam_counts = defaultdict(int)\n",
    "    id_cam_pairs = defaultdict(set)\n",
    "    cam_id_pairs = defaultdict(set)\n",
    "    \n",
    "    images = [f for f in os.listdir(dataset_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    for img in images:\n",
    "        # Try Market1501 pattern first\n",
    "        m = market_pattern.search(img)\n",
    "        if m:\n",
    "            person_id = int(m.group(1))\n",
    "            camera_id = int(m.group(2))\n",
    "        else:\n",
    "            # Try your custom pattern\n",
    "            m = pattern.search(img)\n",
    "            if m:\n",
    "                person_id = int(m.group(1))\n",
    "                camera_id = int(m.group(2))\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        id_counts[person_id] += 1\n",
    "        cam_counts[camera_id] += 1\n",
    "        id_cam_pairs[person_id].add(camera_id)\n",
    "        cam_id_pairs[camera_id].add(person_id)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    id_counts_list = list(id_counts.values())\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{dataset_name} Analysis\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total images: {len(images)}\")\n",
    "    print(f\"Unique person IDs: {len(id_counts)}\")\n",
    "    print(f\"Unique cameras: {len(cam_counts)}\")\n",
    "    \n",
    "    if id_counts:\n",
    "        print(f\"\\nImages per person ID:\")\n",
    "        print(f\"  Average: {np.mean(id_counts_list):.1f}\")\n",
    "        print(f\"  Median: {np.median(id_counts_list):.1f}\")\n",
    "        print(f\"  Min: {min(id_counts_list)}\")\n",
    "        print(f\"  Max: {max(id_counts_list)}\")\n",
    "        print(f\"  Std: {np.std(id_counts_list):.1f}\")\n",
    "        \n",
    "        # Distribution analysis\n",
    "        low_count_ids = [id for id, count in id_counts.items() if count < 5]\n",
    "        medium_count_ids = [id for id, count in id_counts.items() if 5 <= count < 20]\n",
    "        high_count_ids = [id for id, count in id_counts.items() if count >= 20]\n",
    "        \n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(f\"  IDs with < 5 images: {len(low_count_ids)} ({len(low_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        print(f\"  IDs with 5-19 images: {len(medium_count_ids)} ({len(medium_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        print(f\"  IDs with >= 20 images: {len(high_count_ids)} ({len(high_count_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        \n",
    "        # Camera distribution\n",
    "        print(f\"\\nCamera distribution:\")\n",
    "        for cam_id in sorted(cam_counts.keys()):\n",
    "            print(f\"  Camera {cam_id}: {cam_counts[cam_id]} images, {len(cam_id_pairs[cam_id])} unique persons\")\n",
    "        \n",
    "        # Cross-camera analysis\n",
    "        multi_cam_ids = [id for id, cams in id_cam_pairs.items() if len(cams) > 1]\n",
    "        print(f\"\\nCross-camera statistics:\")\n",
    "        print(f\"  IDs appearing in multiple cameras: {len(multi_cam_ids)} ({len(multi_cam_ids)/len(id_counts)*100:.1f}%)\")\n",
    "        \n",
    "        if multi_cam_ids:\n",
    "            cam_counts_per_id = [len(id_cam_pairs[id]) for id in multi_cam_ids]\n",
    "            print(f\"  Average cameras per multi-cam ID: {np.mean(cam_counts_per_id):.1f}\")\n",
    "    \n",
    "    return id_counts, cam_counts, id_cam_pairs\n",
    "\n",
    "def plot_distribution(id_counts, title=\"Images per Person Distribution\"):\n",
    "    \"\"\"Plot histogram of images per person\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts = list(id_counts.values())\n",
    "    plt.hist(counts, bins=50, edgecolor='black')\n",
    "    plt.xlabel('Number of Images')\n",
    "    plt.ylabel('Number of Persons')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def check_data_consistency(train_dir, query_dir, gallery_dir):\n",
    "    \"\"\"Check for data consistency across splits\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Data Consistency Check\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get person IDs from each split\n",
    "    train_ids = set()\n",
    "    query_ids = set()\n",
    "    gallery_ids = set()\n",
    "    \n",
    "    pattern = re.compile(r'(\\d+)_')\n",
    "    \n",
    "    for img in os.listdir(train_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            train_ids.add(int(m.group(1)))\n",
    "    \n",
    "    for img in os.listdir(query_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            query_ids.add(int(m.group(1)))\n",
    "    \n",
    "    for img in os.listdir(gallery_dir):\n",
    "        m = pattern.search(img)\n",
    "        if m:\n",
    "            gallery_ids.add(int(m.group(1)))\n",
    "    \n",
    "    # Check overlaps\n",
    "    train_query_overlap = train_ids & query_ids\n",
    "    train_gallery_overlap = train_ids & gallery_ids\n",
    "    query_gallery_overlap = query_ids & gallery_ids\n",
    "    \n",
    "    print(f\"Train IDs: {len(train_ids)}\")\n",
    "    print(f\"Query IDs: {len(query_ids)}\")\n",
    "    print(f\"Gallery IDs: {len(gallery_ids)}\")\n",
    "    \n",
    "    print(f\"\\nOverlaps (should be non-zero for proper evaluation):\")\n",
    "    print(f\"  Train ∩ Query: {len(train_query_overlap)}\")\n",
    "    print(f\"  Train ∩ Gallery: {len(train_gallery_overlap)}\")\n",
    "    print(f\"  Query ∩ Gallery: {len(query_gallery_overlap)} (should equal Query IDs)\")\n",
    "    \n",
    "    # Check if query IDs are subset of gallery IDs\n",
    "    if query_ids.issubset(gallery_ids):\n",
    "        print(\"✓ All query IDs exist in gallery (correct)\")\n",
    "    else:\n",
    "        missing = query_ids - gallery_ids\n",
    "        print(f\"✗ {len(missing)} query IDs missing from gallery: {list(missing)[:5]}...\")\n",
    "    \n",
    "    return train_ids, query_ids, gallery_ids\n",
    "\n",
    "def generate_reid_statistics(base_dir):\n",
    "    \"\"\"Generate comprehensive statistics for the converted dataset\"\"\"\n",
    "    train_dir = os.path.join(base_dir, \"bounding_box_train\")\n",
    "    query_dir = os.path.join(base_dir, \"query\")\n",
    "    gallery_dir = os.path.join(base_dir, \"bounding_box_test\")\n",
    "    \n",
    "    # Analyze each split\n",
    "    train_stats = analyze_dataset_quality(train_dir, \"TRAIN SET\")\n",
    "    query_stats = analyze_dataset_quality(query_dir, \"QUERY SET\")\n",
    "    gallery_stats = analyze_dataset_quality(gallery_dir, \"GALLERY SET\")\n",
    "    \n",
    "    # Check consistency\n",
    "    check_data_consistency(train_dir, query_dir, gallery_dir)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUMMARY REPORT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    total_images = len(os.listdir(train_dir)) + len(os.listdir(query_dir)) + len(os.listdir(gallery_dir))\n",
    "    print(f\"Total images in dataset: {total_images}\")\n",
    "    \n",
    "    # Save statistics to file\n",
    "    stats_file = os.path.join(base_dir, \"dataset_statistics.txt\")\n",
    "    with open(stats_file, 'w') as f:\n",
    "        f.write(\"CCVID to Market1501 Conversion Statistics\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Total images: {total_images}\\n\")\n",
    "        f.write(f\"Train images: {len(os.listdir(train_dir))}\\n\")\n",
    "        f.write(f\"Query images: {len(os.listdir(query_dir))}\\n\")\n",
    "        f.write(f\"Gallery images: {len(os.listdir(gallery_dir))}\\n\")\n",
    "    \n",
    "    print(f\"\\nStatistics saved to: {stats_file}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/data\"\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    generate_reid_statistics(base_dir)\n",
    "    \n",
    "    # Optional: Plot distributions\n",
    "    # train_dir = os.path.join(base_dir, \"bounding_box_train\")\n",
    "    # train_stats, _, _ = analyze_dataset_quality(train_dir, \"TRAIN SET\")\n",
    "    # plot_distribution(train_stats, \"Train Set: Images per Person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a5ef6",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d46b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-02 16:39:58,919 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2025-08-02 16:39:58,965 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:6.0.0-pyt\n",
      "2025-08-02 16:39:58,974 [TAO Toolkit] [WARNING] nvidia_tao_cli.components.docker_handler.docker_handler 295: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ika/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2025-08-02 16:39:58,974 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 308: Printing tty value True\n",
      "2025-08-02 13:40:01,454 [TAO Toolkit] [INFO] matplotlib.font_manager 1639: generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'ltcc_gpt.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:110: UserWarning: \n",
      "'ltcc_gpt.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "Seed set to 1234\n",
      "Train results will be saved at: /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/results_0.1.4/train\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Train    │      77 │       9576 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Query    │      75 │        493 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │      75 │       7026 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Train    │      77 │       9576 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Query    │      75 │        493 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │      75 │       7026 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/results_0.1.4/train exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model          | Baseline           | 24.1 M | train\n",
      "1 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------\n",
      "24.1 M    Trainable params\n",
      "256       Non-trainable params\n",
      "24.1 M    Total params\n",
      "96.211    Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "Epoch 59:  93%|█████████▎| 553/596 [00:29<00:02, 19.05it/s, v_num=1, train_loss_step=0.848, train_loss_epoch=0.933, base_lr=0.001]`Trainer.fit` stopped: `max_epochs=60` reached.\n",
      "Epoch 59:  93%|█████████▎| 553/596 [00:30<00:02, 18.14it/s, v_num=1, train_loss_step=0.848, train_loss_epoch=0.933, base_lr=0.001]2025-08-02 14:08:54,292 [TAO Toolkit] [WARNING] root 339: Telemetry data couldn't be sent, but the command ran successfully.\n",
      "2025-08-02 14:08:54,292 [TAO Toolkit] [WARNING] root 342: [Error]: 'str' object has no attribute 'decode'\n",
      "2025-08-02 14:08:54,293 [TAO Toolkit] [INFO] root 349: Execution status: PASS\n",
      "2025-08-02 17:08:55,087 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 371: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao model re_identification train -e /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/ltcc_gpt.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ace39",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6e1172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-02 10:30:07,788 [TAO Toolkit] [INFO] root 160: Registry: ['nvcr.io']\n",
      "2025-08-02 10:30:07,836 [TAO Toolkit] [INFO] nvidia_tao_cli.components.instance_handler.local_instance 360: Running command in container: nvcr.io/nvidia/tao/tao-toolkit:6.0.0-pyt\n",
      "2025-08-02 10:30:07,847 [TAO Toolkit] [WARNING] nvidia_tao_cli.components.docker_handler.docker_handler 295: \n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/ika/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "2025-08-02 10:30:07,847 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 308: Printing tty value True\n",
      "2025-08-02 07:30:11,206 [TAO Toolkit] [INFO] matplotlib.font_manager 1639: generated new fontManager\n",
      "sys:1: UserWarning: \n",
      "'ltcc.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/hydra/hydra_runner.py:110: UserWarning: \n",
      "'ltcc.yaml' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  _run_hydra(\n",
      "/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "/usr/local/lib/python3.12/dist-packages/nvidia_tao_pytorch/core/loggers/api_logging.py:236: UserWarning: Log file already exists at /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/results_0.0.1/evaluate/status.json\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]Evaluate results will be saved at: /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/results_0.0.1/evaluate\n",
      "╒══════════╤═════════╤════════════╤═════════════╕\n",
      "│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Query    │      75 │        493 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │      75 │       7026 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "\n",
      "Testing: |          | 0/? [00:00<?, ?it/s]╒══════════╤═════════╤════════════╤═════════════╕│ Subset   │   # IDs │   # Images │   # Cameras │\n",
      "╞══════════╪═════════╪════════════╪═════════════╡\n",
      "│ Query    │      75 │        493 │          12 │\n",
      "├──────────┼─────────┼────────────┼─────────────┤\n",
      "│ Gallery  │      75 │       7026 │          12 │\n",
      "╘══════════╧═════════╧════════════╧═════════════╛\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 59/59 [00:05<00:00, 10.10it/s]The test features are normalized.The distance matrix is computed using euclidean distance. It is then processed by re-ranking.\n",
      "╒════════════════════╤═════════╕\n",
      "│ Name               │ Score   │\n",
      "╞════════════════════╪═════════╡\n",
      "│ mAP                │ 3.5%    │\n",
      "├────────────────────┼─────────┤\n",
      "│ CMC curve, Rank-1  │ 5.3%    │\n",
      "├────────────────────┼─────────┤\n",
      "│ CMC curve, Rank-5  │ 15.0%   │\n",
      "├────────────────────┼─────────┤\n",
      "│ CMC curve, Rank-10 │ 21.5%   │\n",
      "╘════════════════════╧═════════╛\n",
      "\n",
      "Testing DataLoader 0: 100%|██████████| 59/59 [00:12<00:00,  4.81it/s]2025-08-02 07:30:32,198 [TAO Toolkit] [WARNING] root 339: Telemetry data couldn't be sent, but the command ran successfully.\n",
      "2025-08-02 07:30:32,198 [TAO Toolkit] [WARNING] root 342: [Error]: 'str' object has no attribute 'decode'\n",
      "2025-08-02 07:30:32,198 [TAO Toolkit] [INFO] root 349: Execution status: PASS\n",
      "2025-08-02 10:30:33,140 [TAO Toolkit] [INFO] nvidia_tao_cli.components.docker_handler.docker_handler 371: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "!tao model re_identification evaluate \\\n",
    "    -e /home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/ltcc.yaml \\\n",
    "    evaluate.checkpoint=/home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/results_0.1.1/train/model_epoch_019_step_00901.pth \\\n",
    "    evaluate.query_dataset=/home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/data/query \\\n",
    "    evaluate.test_dataset=/home/ika/yzlm/TwinProject/ReID_Experiments/LTCC_ReID/data/bounding_box_test  \\\n",
    "    re_ranking.re_ranking=True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
